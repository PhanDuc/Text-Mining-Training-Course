{"link": "https://techcrunch.com/2016/11/19/how-data-science-and-rocket-science-will-get-humans-to-mars/", "content": "\n In a recent\u00a0  to CNN, President Obama re-affirmed America\u2019s commitment to sending a manned mission to Mars. Think your data science challenges are too complicated? Imagine the difficulties involved in mining data to understand the health impacts of an expedition to Mars. What happens to astronauts\u2019 muscle tone or lung capacities after several years in space? How much weight can they safely lose? How much CO2 should be in the crew vehicle? How many sensors are needed to calculate joint flexibility in each individual space suit? When sending humans \u201cwhere no one has gone before,\u201d there are a multitude of variables to consider, and NASA is hard at work   of a future Mission to Mars. Understanding these risks is critical, as they impact a number of decisions that need to be made when planning the journey \u2014 spanning everything from how potential crew members are evaluated to equipment engineering, mission logistics and the determination of needed fuel loads. The stakes are high, but NASA realized from the get-go that it needed to focus less on developing the perfect analytic model and more on building a data science   that empowers decision-makers to use analytics to answer a multitude of continually changing questions. But you don\u2019t have to be dealing with rocket science to learn from NASA\u2019s analytic approach. Here are several key takeaways from NASA\u2019s project that are useful for any organization about to embark \u2014 or that\u2019s stuck \u2014 on a big data analytics initiative. Simply put, data science shouldn\u2019t be as complicated as rocket science. (See what I did there?) Yes, analyzing big data has challenges, and yes, your approach may vary depending on what kinds of insights you hope to obtain, but there\u2019s no need to make things more complex than the situation calls for. All too often, organizations end up spending endless cycles attempting to move data in order to analyze it when they should instead be focusing on bringing the analytics to the data. Big data, by definition, is very tough, if not impossible, to move around. This is why distributed storage and processing frameworks like Hadoop exist \u2014 data in the cloud is far more scalable than data in a silo. For the Mars project, there are so many levels of data to look at, ranging from health data collected from astronauts like   who have completed previous space missions, to non-astronaut test studies, to studies done in simulated space environments like the   (HERA) at Johnson Space Center in Houston. Getting all the data in one place is the critical first step. For this reason, NASA is using the   platform developed by Lockheed Martin and several analytic partners, such as Alpine Data, to analyze data at its source. Because there\u2019s no waiting to download data into a separate analytic environment to work with it, researchers can focus their time and energy on asking questions and getting the answers that will help them plan a mission to Mars. A successful rocket launch is only step one in a multi-year expedition to Mars. Based on past experiences, NASA expects to encounter and address numerous challenges along the way. The same holds true for data analytics projects. Simply deploying a model doesn\u2019t mean the project is done. In fact, the most valuable analytics initiatives are those where models are continually refined and iterated on an ongoing basis. Like the scientific method, getting the most out of analytics requires experimentation, testing, learning from failures and testing again. NASA wants to be able to quickly query the large volumes of data at its disposal, then funnel insights back into new models capable of building on what came before. That\u2019s why the data science process for this initiative resembles a \u201cpendulum,\u201d where the forward swing focuses on rapidly driving insights out to researchers and the backward swing focuses on measuring, evaluating results, refining the model and then swinging again. An ability to quickly and easily refine analytic models is especially valuable when your data sets aren\u2019t perfect. (And really, is any data set perfect?) For NASA, the biggest data challenge is that the astronaut sample size is small \u2014 only 300 individuals have been\u00a0accepted\u00a0to NASA\u2019s Astronaut Corps. Researchers have to mine the heck out of the data collected from this small sample and extrapolate. For example, based on how a 35-year-old female with a starting weight of 120 pounds responded to a five-month trip in space, how would a 32-year-old weighing 123 respond to two years? What about a 30-year-old weighing 118? Furthermore, since an astronaut has yet to step foot on the Red Planet, there\u2019s no data about the health impacts of actually living on Mars (Matt Damon doesn\u2019t count). But what can NASA learn from astronauts who have gone to the Moon, or spent a year in the International Space Station? What happens when data from test subjects living in simulated space environments is plugged into a predictive model? With analytic tools that support rapid model deployment and refinement, organizations can keep trying different ways to extract insight from the data they   have to make better predictions, even when key information is missing. With the Mars Mission, NASA is not only putting billions of invested taxpayer dollars on the line, but also the lives of its astronauts, who risk their health and safety in the name of science and exploration. Like any consumer of analytics, NASA needs to be able to trust in the recommendations that are being generated, but this is hard to do if predictions are computed in a \u201cblack box\u201d that only data science experts can manipulate or understand. For a project like this, empowering analytic consumers who aren\u2019t necessarily data science PhDs (such as the health researchers, equipment engineers and others planning the mission) to actually build and launch queries and use the data on their own is key. This requires tight collaboration between business and IT stakeholders, modeling tools that are simple to use and modify and the ability to push insights to the people who need them. This is why NASA has chosen a collaborative analytic platform that includes tools that extend outputs directly into the systems and applications that are used by the scientists and decision-makers working on the Mars Mission. Large and complex data sets pose challenges for any organization about to embark on an analytics deployment. But NASA\u2019s example of harnessing data to plan the most complicated of journeys \u2014 an expedition to Mars \u2014 proves that the challenges are not insurmountable. With the right tools and, most importantly, a consistent and well-planned approach, data science doesn\u2019t have to be as daunting as rocket science."}
{"link": "https://techcrunch.com/2016/11/16/microsoft-offers-concessions-to-eu-regulators-eyeing-its-26-2bn-linkedin-bid/", "content": "Microsoft has offered\u00a0concessions to EC competition regulators in the hopes of smoothing the way for its planned acquisition of LinkedIn. It announced the  . \u00a0reports the development follows a meeting between the Commission and Microsoft executives last week at which the EC expressed concerns. An EC spokesperson confirmed it has received \u201ca commitments proposal\u201d from Microsoft but declined to comment further. Commitments, in EC antitrust nomenclature, refers to\u00a0remedies a company can offer if the Commission has concerns a\u00a0merger/acquisition may significantly affect competition. These\u00a0can include proposing certain modifications to the project to\u00a0guarantee continued competition on the market. A Microsoft spokesperson declined to comment on what\u00a0concessions\u00a0it is offering. Nor would it comment more generally on the big data competition issue. But the spokesman did point to  , written by competition lawyer  \u00a0(whose firm has advised Microsoft on a range of antitrust issues), which sets out arguments against\u00a0big-data-related competition concerns as a blocker to\u00a0the Microsoft-LinkedIn\u00a0acquisition. Kinsella\u2019s position is the big data competition concern essentially pivots on the question of whether a data set can be \u201cunmatched.\u201d \u201cUnless there is convincing evidence that a particular data set is genuinely both non-replicable and uncontestable, it would place an unreasonable burden on competition enforcers if they were always obliged to analyse the impact on some rather nebulous \u2018data market,\u2019 \u201d he writes. Complaints that the LinkedIn acquisition might make it difficult for Microsoft\u2019s competitors to launch future services are dismissed by Kinsella\u00a0as moving into \u201cthe realm of an \u2018innovation offence\u2019 \u201d \u2014 where\u00a0regulators would be tackling, in his words, \u201cspeculative impact\u201d and \u201cthe levels of abstraction introduce too much uncertainty into the merger review process.\u201d Clearly Microsoft hopes the Commission will come to similar conclusions. The majority of competition cases are cleared unconditionally by the EC, so the Microsoft-LinkedIn deal is already in a minority \u2014 although the EC spokesman said\u00a0it\u00a0also has\u00a0\u201cmany cases\u201d where parties submit remedies at this stage as they seek to satisfy\u00a0regulator concerns. For more than a year now EU Competition Commissioner Margrethe Vestager has raised concerns about\u00a0whether big data players\u2019 data holdings can impact competition,  \u00a0\u201cIf\u00a0a\u00a0company\u2019s use of\u00a0data is so bad for competition that it outweighs the\u00a0benefits,\u00a0we may have to step in to restore\u00a0a level playing field.\u201d Marc Benioff, chief executive officer of Salesforce.com \u2014 which\u00a0also made a bid for\u00a0LinkedIn \u2014 has been one of the most\u00a0 , arguing Microsoft will use LinkedIn\u2019s database\u00a0of ~430 million professional profiles of users to \u201cblock competition.\u201d For its part,\u00a0  it wants to integrate LinkedIn data into its existing software programs, such as\u00a0Outlook email, Skype messaging and Office\u2019s suite of productivity apps \u2014 combining the social graph of\u00a0the professional network with data held within\u00a0Microsoft users\u2019 emails, calendars and comms in order to enable new utility and functionality.\u00a0Such as\u00a0\u201ca LinkedIn newsfeed that serves up articles based on the project you are working on and Office suggesting an expert to connect with via LinkedIn to help with a task you\u2019re trying to complete.\u201d It has also said it wants to apply machine learning/AI on top of that mix to, in the  , \u201ccreate the ultimate selling tool, the ultimate customer support tool.\u201d So another driver for the LinkedIn acquisition is its vast social graph\u00a0dataset as a pure \u201craw material\u201d for training the machine learning models Microsoft\u00a0will need to build a next generation of smarter software. AI\u2019s need\u00a0for data\u00a0to create new products likely helps explain the hefty price-tag on the LinkedIn acquisition. But it remains to be seen how EU regulators will weigh up the competition implications\u00a0of any such LinkedIn-data-enabled future Microsoft products. The new deadline for the EC\u2019s\u00a0decision on the acquisition is\u00a0 ."}
{"link": "https://techcrunch.com/2013/08/15/panorama-education-wants-to-make-polling-parents-students-and-teachers-easier-for-educators/", "content": "New Haven-based startup and current Y Combinator Summer 2013 participant   is looking to address a major pain point for educators, students and parents around school with its polling app targeted at K-12 students. The technology is designed to replace cumbersome legacy players, and improve on less targeted, general-purpose tools like SurveyMonkey to really help survey provide meaningful insights for education. Panorama came out of Yale alumni Aaron Feuer\u2019s own personal frustrations with conducting surveys of classes. Feuer teamed up with fellow Yale students David Carel and Xan Tanner to create Panorama, drawing on their expertise with tracking and analyzing other types of data (sports) and their experience with the education system. \u201cWe\u2019re helping schools measure things, gather feedback and then use that data to improve,\u201d Feuer said in an interview. \u201cThe big reason schools use us over SurveyMonkey is that we help them figure out what to ask, and we help them figure out what to do with the information. Tools like SurveyMonkey are great to just tell you the answers to whatever your surveying someone about, but if you want to understand what that actually means and how to interpret it, and you want to look at it in context with other data than you need something like Panorama.\u201d Though it\u2019s new to YC, Panorama Education isn\u2019t new entirely. The team started the company just before their senior year of college and have managed to sell their product into 3,000 schools in the U.S., and more than 500 abroad. Their annual recurring revenue run rate is currently at around $500,000, which is especially impressive as they only offer a paid product to the education market, which is traditionally reluctant about making budget bets on new products. During their time in YC, they\u2019ve managed to increase their customer base by over 100 percent, and that\u2019s a key goal they had coming into the program. But their ambition goes beyond that, Feuer says. \u201cSo so far our main focus has been running surveys in schools, and we can launch a huge business just on that,\u201d he said. \u201cBut what\u2019s more exciting to us is coming back to the idea that schools don\u2019t use data. If you\u2019re in a business, the only way to improve is to look at customer retention, revenues, sales forecasts and all this data, but schools don\u2019t do that. Our mission it to transform that, by going from doing surveys and looking at that data, to looking at all the data schools collect and helping them devise smart ways to use that data and their available tools.\u201d Harnessing big data for education is a hot and growing area, but one that\u2019s still very much like the Wild West. Panorama, as its name implies, wants to become a high-level view that helps educators sort through that tangled mess and start driving more value for teachers, rather than making them pore through endless sheets of numbers. It\u2019s a big idea, but when a company has paying clients on its platform already buying in, that\u2019s a good sign."}
